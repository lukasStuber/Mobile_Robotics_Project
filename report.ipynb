{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobile Robotics Project - Final Report\n",
    "\n",
    "Group Members:\n",
    "- ESCOYEZ, Antoine Jacques Richard, 335564\n",
    "- POUSSIN, Jean-Baptiste Marie Alexandre, 303127\n",
    "- KOCKISCH, Matthias Hugues JÃ¶rg, 303000\n",
    "- STUBER, Lukas, 289304\n",
    "\n",
    "Professor:\n",
    "\n",
    "Prof. MONDADA, Francesco\n",
    "\n",
    "\n",
    "EPFL, 11.12.2022\n",
    "\n",
    "Course MICRO-452 Basics of Mobile Robotics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "The goal of the project in the Basics of Mobile Robotics class was to control the two sheel Thymio robot. The environment of the robot is a map designed by the group members. On the map there need to be obstacles into which the robot cannot/must not drive. According to the chosen map and the obstacles present on the map a global path from the start to the goal needs to be calculated using computer vision. The start and initial position (*i.e.* the location of Thymio,) can be chosen by the user. On its path to the goal the robot must be be able to detect and avoid unforeseen obstacles that are added by the user. For better tracking of the path some kind of filtering must be used that improves the state estimation of the robot. The camera image serves as measurement for the filter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed Description of our Choices\n",
    "Our group chose the map to be a parking space with two rows of parking lots. The yellow lines marking the edges of the parking lots are the fixed obstacles used in the global path planning. The robot is neighter allowed to cross the yellow lines nor leave the map. The camera is fixed above the map and oversees the entire situation (robot, lines, free spaces, occupied spaces). To facilitate the detection of the position and orientation of the robot in the images we sticked two coloured stickers on the top side of the robot. The left sticker is green, the right one is blue. The computer vision algorithm uses these two points to determine the center of the wheels axis and the orientation of the robot. The goal is a free parking lot indicated by a red paper square. The unknown obstacles (unknown to/ignored by the computer vision) are <font color='red'>????</font>\n",
    ". The estimated state correction is done with the help of a Kalman filter whose measurement is the location of the Thymio in the camera images.\n",
    "\n",
    "\n",
    "<img src=\"Map.jpg\" width=\"300\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n",
    "We use a *AUKEY 1080P - 30 pfs* camera as the optical sensor for the computer vision. It is fixed on a stand (a lamp) and sees the entire map from above. Its position does not change during the operation of the robot.\n",
    "\n",
    "To enable a clear and stable detection of the objects on the map a series of adjustments must be done before the object detection. The image processing is done in HSV representation of the image. The procedure is automatically started when the main function is executed. It includes:\n",
    "- Adjust the luminosity <font color='red'>????</font>\n",
    "- Detection of the outer borders of the map\n",
    "- Adjust the red upper and lower thresholds <font color='red'>????</font>\n",
    "- Adjust the green upper and lower thresholds <font color='red'>????</font>\n",
    "- Adjust the blue upper and lower thresholds <font color='red'>????</font>\n",
    "- Adjust the yellow upper and lower thresholds <font color='red'>????</font>\n",
    "\n",
    "<font color='red'>Describe the steps of the computer vision to find the colorized map.</font>\n",
    "\n",
    "The computer vision algorithm abstracts the picture of the map leaving only the relevant objects (map surface, robot (green and blue spot), obstacle lines, goal) in full colors (rgb values eighter 255 or 0)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Path Planning\n",
    "The global path planning is done using the A* algorithm, as seen in Exercise 5 of the course. It uses the simplified map generated by the computer vision part as input. The typical size of this image is <font color='red'>????</font>. Obviously, this is far too large for the A* algorithm to finish in an acceptable time. To reduce the size of the image a convolutional filter is applied to the pixels. Its size is can be chosen by the user and is represented by the variable \"kernel\". The filter devides the image from the vision part into patches of size 2*kernel+1 by 2*kernel+1. The output pixel is 0 if:\n",
    "- all the pixels in the patch are black (free space)\n",
    "- at least one of the pixels is green or blue (robot position)\n",
    "- at least one of the pixels is red (goal position)\n",
    "\n",
    "The output is 1 if:\n",
    "- at least one of the pixels is yellow (obstacle line)\n",
    "\n",
    "The result of this filtering is a map of typical size 120 by 60 pixels (depending on the size of the input map) where the free, start and goal space is 0 and the obstacle space is 1.\n",
    "\n",
    "This reduced map and the start and goal positions are fed into the A* algorithm that calculates the optimal global path from the start to the goal. The calculated path is a series of coordinates in the reduced map format. To serve as directions for the Thymio the coordinates need to be resized to the original map (multiplied by the reduction factor 2*kernel+1)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Control\n",
    "Input: path (series of coordinates)\n",
    "Output: speed of the wheels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Filter\n",
    "The Kalmna Filter is quite standard. It is a class with the following attributes:\n",
    "- the current state estimation, a vector with (x position, y position, orientation theta)\n",
    "- the current state estimation covariance matrix\n",
    "- the process uncertainty matrix (constant, describes the noise of the state propagation)\n",
    "- the measurement uncertainty matrix (constant, describes the noise of the measurement by the camera)\n",
    "- the observation matrix (constant, maps the state to the observation of the state), since all of the states can be measured, this is an identity matrix\n",
    "- the time stamp of the last time the filter was called (used for calculating the elapsed time)\n",
    "\n",
    "and the following methods:\n",
    "- state_prop <br>\n",
    "It takes as input the speeds of the two wheels during the last timestep. From that it estimates the new position and state covariance matrix and updates the corresponding attributes. Because the state propagation includes sinus and cosinus calculations, it is not a linear system. To facilitate the calculations a first order approximation is used that linearizes the system. The error of the approximation is limited because the state_prop function is called every 25ms <font color='red'>????</font>. So, the involved angle is very small and so is the error of the linearization. \n",
    "- state_correct <br>\n",
    "It takes as input the measurement of the position and angle of the robot from the camera. It calculates the Kalman gain and corrects the state and state covariance. It takes quite some time to take an image with the camera and extract the position of the robot, which means that the correct function can only be called every 1.5s <font color='red'>????</font>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local obstacles avoidance\n",
    "Input: prox measurements\n",
    "Output: speed of the wheels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function\n",
    "bla bla, maybe talk about the timers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "conclude conclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "\n",
    "from __future__ import print_function\n",
    "import cv2 as cv\n",
    "from parking_segmentation import *\n",
    "from color_segmentation import *\n",
    "from color_centroids import *\n",
    "from discretize_map import *\n",
    "from control import ThymioControl\n",
    "from Kalman import Kalman\n",
    "from constants import *\n",
    "from RepeatedTimer import RepeatedTimer\n",
    "\n",
    "# IMAGE PROCESSING\n",
    "id_camera = 1\n",
    "##[Parking segmentation]\n",
    "corners, destination_corners = set_parking_limits(id_camera)\n",
    "##[Color segmentation]\n",
    "segmentation, refined_color_dict_HSV, kernels, openings = get_color_mask(id_camera, corners, destination_corners, real_size=(NOMINAL_AREA_LENGTH, NOMINAL_AREA_WIDTH))\n",
    "cv.namedWindow(\"Segmentation Result\")\n",
    "cv.imshow(\"Segmentation Result\", segmentation)\n",
    "key = cv.waitKey(0)\n",
    "cv.destroyWindow(\"Segmentation Result\")\n",
    "##[Thymio and objective localization]\n",
    "centroids = {'goal': (0, 0), 'thymio': (0, 0), 'green': (0, 0), 'blue': (0, 0)}\n",
    "theta_thymio = 0\n",
    "localization = None\n",
    "\n",
    "kalman = Kalman()\n",
    "thymio = ThymioControl()\n",
    "\n",
    "def compute_centroids():\n",
    "    global centroids, theta_thymio, localization\n",
    "    centroids, theta_thymio, localization = get_centroids(id_camera, corners, destination_corners, refined_color_dict_HSV, kernels, openings, prev_centroids=centroids, real_size=(NOMINAL_AREA_LENGTH, NOMINAL_AREA_WIDTH), real_time=False)\n",
    "    #print(\"centroids at\", centroids['thymio'][0], centroids['thymio'][1])\n",
    "    #print(\"thymio angle at \", theta_thymio)\n",
    "    kalman.state_correct(np.array([centroids['thymio'][0], centroids['thymio'][1], theta_thymio]))\n",
    "    thymio.position = (kalman.x[0, 0], kalman.x[1, 0])\n",
    "    thymio.angle = kalman.x[2, 0]\n",
    "\n",
    "def odometry():\n",
    "    kalman.state_prop(thymio.speed_target)\n",
    "    thymio.position = (kalman.x[0, 0], kalman.x[1, 0])\n",
    "    thymio.angle = kalman.x[2, 0]\n",
    "    #print(thymio.position[0], thymio.position[1], thymio.angle*180/math.pi)\n",
    "\n",
    "# def plot_localization():\n",
    "#     global localization\n",
    "#     cv.namedWindow(\"Localization Result\")\n",
    "#     cv.imshow(\"Localization Result\", localization)\n",
    "#     key = cv.waitKey(30)\n",
    "#     cv.destroyWindow(\"Localization Result\")\n",
    "\n",
    "# initialise position and path\n",
    "compute_centroids()\n",
    "kalman.set_state((centroids['thymio'][0], centroids['thymio'][1], theta_thymio))\n",
    "thymio.position = (centroids['thymio'][0], centroids['thymio'][1])\n",
    "thymio.angle = theta_thymio\n",
    "path = discretize_map(segmentation, centroids)\n",
    "thymio.set_path(path)\n",
    "\n",
    "# start updating position and follow path\n",
    "image_timer = RepeatedTimer(1.5, compute_centroids)\n",
    "odometry_timer = RepeatedTimer(ODOMETRY_INTERVAL, odometry)\n",
    "image_timer.start()\n",
    "odometry_timer.start()\n",
    "thymio.follow_path()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5ea979ccd3d451dccb2f53410d86247b09f6658a9c231229e1b64835c7f7f88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
