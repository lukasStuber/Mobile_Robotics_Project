{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobile Robotics Project - Final Report\n",
    "\n",
    "Group Members:\n",
    "- ESCOYEZ, Antoine Jacques Richard, 335564\n",
    "- POUSSIN, Jean-Baptiste Marie Alexandre, 303127\n",
    "- KOCKISCH, Matthias Hugues JÃ¶rg, 303000\n",
    "- STUBER, Lukas, 289304\n",
    "\n",
    "Professor:\n",
    "\n",
    "Prof. MONDADA, Francesco\n",
    "\n",
    "\n",
    "EPFL, 11.12.2022\n",
    "Course MICRO-452 Basics of Mobile Robotics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "The goal of the project of Basics Mobile Robotics class was the control of a two wheeled Thymio robot. The environment of the robot is a map designed by the group members. On the map there need to be obstacles into which the robot must not drive. According to the map and certain fixed obstacles present on the map, a global path from the start to the goal must be calculated using computer vision from an external camera. The start is the initial location of the Thymio, the goal can be placed by the user. On its path to the goal the robot needs to be able to detect and avoid unforeseen obstacles that are added manually. For better tracking of the path some kind of filtering must be used to improve the state estimation of the robot. The camera image serve as measurements for the latter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed Description of our Choices\n",
    "We chosed the map to be a parking with two rows of parking lots arround and between which the Thymio will move. \n",
    "\n",
    "The yellow lines mark the edges of the parking lots and are the fixed obstacles used in the global path planning. The robot is neither allowed to cross the yellow lines nor leave the map while following this path. The camera is fixed above the map and oversees the entire situation (robot, lines, free spaces, occupied spaces). \n",
    "\n",
    "In order to detect the position and orientation of the robot, we sticked a green and a blue stickers on the top of it, right above its wheels. The computer vision algorithm uses these two points to determine the center of the wheels axis and the orientation of the robot. The goal is a free parking space indicated by a red square paper. The unexpected obstacles (ignored by the computer vision) are some objects that we add on Thymio's path. There is also an other Thymio covered by a yellow paper (i.e. considered at first as a global obstacle because of the yellow color) that we control remotely to get out of its parking lot and get in the Thymio's path, as if it was a bad driver. The obstacle avoidance will then avoid this bad driver, as in assisted driving. \n",
    "\n",
    "The estimated state correction is done with the help of a Kalman filter whose measurement is the location of the Thymio in the camera images.\n",
    "\n",
    "<img src=\"Map.jpg\" width=\"300\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n",
    "\n",
    "We use an *AUKEY 1080P - 30 fps* camera as the input for the computer vision. It is fixed on a stand (a lamp) and sees the entire map from above. Its position does not change during the operation of the robot.\n",
    "\n",
    "The image processing is done with thresholding in HSV space combined with mathematical morphology. The processing includes several steps controlled by different popping windows when the main.py function is called:\n",
    "- Crop parking from camera frame\n",
    "- Segment yellow lines and/or fixed obstacles of the parking space\n",
    "- Segment red goal\n",
    "- Segment green and blue patches to detect Thymio location and orientation\n",
    "- Send the computed map with color masks as input to A*\n",
    "- Compute Thymio and goal centroids in real time for Kalman filtering\n",
    "\n",
    "This procedure involves 3 differents scripts: **parking_segmentation.py**, **color_segmentation.py** and **color_centroids.py** which share common features\n",
    "\n",
    "### Rescaling of the image\n",
    "\n",
    "```python\n",
    "def rescale_frame(frame, scale_percent):\n",
    "    width = int(frame.shape[1] * scale_percent / 100)\n",
    "    height = int(frame.shape[0] * scale_percent / 100)\n",
    "    dsize = (width, height)\n",
    "    return cv.resize(frame, dsize)\n",
    "```\n",
    "We decided to keep only 50% of the pixels to make the computation more efficient while keeping a minimum of details. At first we used only 20% and we had some issues to segment small blue and green patches or even to isolate the parking when the lighting conditions were not perfect.\n",
    "\n",
    "### HSV thresholding\n",
    "\n",
    "https://docs.opencv.org/3.4/da/d97/tutorial_threshold_inRange.html\n",
    "\n",
    "Typically, all different scripts involve such an operation\n",
    "\n",
    "```python\n",
    "frame_HSV = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "frame_threshold = cv.inRange(frame_HSV, (low_H, low_S, low_V), (high_H, high_S, high_V))\n",
    "```\n",
    "whose values are controlled by trackbars on a window.\n",
    "\n",
    "```python\n",
    "cv.createTrackbar(low_H_name, window_detection_name , low_H, max_value_H, on_low_H_thresh_trackbar)\n",
    "\n",
    "def on_low_H_thresh_trackbar(val):\n",
    "    global low_H\n",
    "    global high_H\n",
    "    low_H = val\n",
    "    low_H = min(high_H-1, low_H)\n",
    "    cv.setTrackbarPos(low_H_name, window_detection_name, low_H)\n",
    "```\n",
    "\n",
    "This allowed us to be more robust to different lighting conditions.\n",
    "\n",
    "After the mask from thresholding are computed we perform mathematical morphology, either erosion only when we want to segment the parking or we choose between opening and closing when we segment the different colors (yellow obstacles and lines, red goal and patches on the Thymio for localization).\n",
    "\n",
    "**parking_segmentation.py**\n",
    "```python\n",
    "kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "mask = cv.erode(frame_threshold, kernel)\n",
    "```\n",
    "\n",
    "For the parking segmentation we only use erosion because we want to remove details at the edges of our map.\n",
    "\n",
    "**color_segmentation.py**\n",
    "```python\n",
    "kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "if opening:\n",
    "    color_mask = cv.erode(color_mask, kernel)\n",
    "    color_mask = cv.dilate(color_mask, kernel)\n",
    "else:\n",
    "    color_mask = cv.dilate(color_mask, kernel)\n",
    "    color_mask = cv.erode(color_mask, kernel)\n",
    "```\n",
    "\n",
    "For the color patches:\n",
    "- if the patch is big (yellow lines, goal) we more likely want to remove edge details and smooth using opening\n",
    "- if the patch is small, we want to fill gaps and crop inflations with closing.\n",
    "\n",
    "In both cases, ``kernel_size`` is also controlled by a trackbar on the window.\n",
    "For the color patches, we keep track of the previous color masks to get a preview of the final color mask.\n",
    "The computer vision algorithm abstracts the picture of the map leaving only the relevant objects in solid colours: map surface (black), robot (green and blue spots), obstacle lines (yellow), goal (red).\n",
    "\n",
    "<img src=\"color_segmentation.png\" width=\"300\">\n",
    "\n",
    "### Contours detection and homography\n",
    "\n",
    "<img src=\"parking_segmentation.png\" width=\"300\">\n",
    "\n",
    "For the parking segmentation, the mask is very coarse.\n",
    "In order to remove the noise we identify the biggest contour which corresponds to the black area of the parking.\n",
    "\n",
    "Once we have the contour we compute the smallest rectangle containing the contour to display in green in the window when we are setting our parameters to make sure we are thresholding the parking.\n",
    "\n",
    "```python\n",
    "contours, _ = cv.findContours(mask, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "cnt = max(contours, key=cv.contourArea)\n",
    "x, y, w, h = cv.boundingRect(cnt)\n",
    "cv.rectangle(frame,(x, y),(x + w, y + h),(0, 255, 0), 2)\n",
    "\n",
    "corners = order_points(cnt)\n",
    "destination_corners = find_dest(corners)\n",
    "```\n",
    "\n",
    "The order_points function outputs top-left, top-right, bottom-right and bottom-left points from a contour list.\n",
    "The destination_corners function computes maximum height and width from the previous quadrilateral.\n",
    "The idea is simple, we know that the plan of the camera may not be parallel to the plan of the floor where our parking lies. So we want to project the contour corners on a rectangle as a classic OCR tool would for a scanned document.\n",
    "https://learnopencv.com/automatic-document-scanner-using-opencv/\n",
    "Once we have our corners and we know were to project we can perform perspective transform with homography.\n",
    "\n",
    "\n",
    "```python\n",
    "M = cv.getPerspectiveTransform(np.float32(corners), np.float32(destination_corners))\n",
    "cropped_frame = cv.warpPerspective(frame, M, (destination_corners[2][0], destination_corners[2][1]), flags=cv.INTER_LINEAR)\n",
    "```\n",
    "\n",
    "### Extra morphology for fixed obstacles\n",
    "\n",
    "We decided to inflate the mask for yellow objects (fixed obstacles and lines) to encourage A* to produce a path with a safety margin from the obstacles.\n",
    "\n",
    "```python\n",
    "h, l = cropped_frame.shape[:2]\n",
    "L, H = real_size\n",
    "inflate_pixel = (int((2 * INFLATE_WIDTH + 1) * l/L), int((2 * INFLATE_LENGTH + 1) * h/H))\n",
    "if color == \"yellow\":\n",
    "    color_mask = cv.dilate(color_mask, np.ones(inflate_pixel, np.uint8))\n",
    "```\n",
    "\n",
    "We forced a security margin of 45 mm which corresponds to almost half of the Thymio width so he would park at the center of a parking space and not go over the lines.\n",
    "\n",
    "### Compute goal and Thymio centroids\n",
    "\n",
    "<img src=\"centroids.png\" width=\"300\">\n",
    "\n",
    "From the blue and green patches we are able to compute the centroid of the Thymio and its orientation.\n",
    "Indeed, the color patches are placed right above the wheels and the color difference gives a unique orientation.\n",
    "\n",
    "```python\n",
    "if np.argwhere(color_masks['red'] == 255).sum() == 0:\n",
    "    y_goal, x_goal = y_goal_prev, x_goal_prev\n",
    "else:\n",
    "    y_goal, x_goal = np.argwhere(color_masks['red'] == 255).mean(axis=0).astype(int)\n",
    "if (np.argwhere(color_masks['green'] == 255).sum() == 0) or (np.argwhere(color_masks['blue'] == 255).sum() == 0):\n",
    "    y_thymio, x_thymio = y_thymio_prev, x_thymio_prev\n",
    "else:\n",
    "    y_green, x_green = np.argwhere(color_masks['green'] == 255).mean(axis=0).astype(int)\n",
    "    y_blue, x_blue = np.argwhere(color_masks['blue'] == 255).mean(axis=0).astype(int)\n",
    "    y_thymio, x_thymio = int((y_green + y_blue)/2), int((x_green + x_blue)/2)\n",
    "```\n",
    "\n",
    "At each step we save the positions of the previous centroids so we can use them if the color patches aren't visible in a given frame. We send those centroids and orientation to the Kalman filter after a conversion from pixels to mm and plot it in real time as shown above.\n",
    "\n",
    "```python\n",
    "def indices_to_mm(real_size, cropped_frame, centroid, reverse=False):\n",
    "    L, H = real_size\n",
    "    h, l = cropped_frame.shape[:2]\n",
    "    x, y = centroid\n",
    "    if reverse:\n",
    "        return (int(h*x/H), int(l*y/L))\n",
    "    return (int(H*x/h), int(L*y/l))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Path Planning\n",
    "The global path planning is done using the A* algorithm, as seen in Exercise 5 of the course. It uses the simplified map generated by the computer vision part as input. The typical size of this image is 1600 by 800 pixel, which is far too large for the A* algorithm to finish in an acceptable time. To reduce the size of the image, a convolutional filter is applied to the pixels. Its size (the variable ``kernel``) can be chosen by the user. The filter divides the image from the vision part into square patches of side length $2\\:\\times$ ``kernel`` $+\\:1$.<br>\n",
    "\n",
    "<img src=\"segmentation_kernel.png\" width=\"300\">\n",
    "\n",
    "The output pixel is 0 if:\n",
    "- all the pixels in the patch are black (free space)\n",
    "- at least one of the pixels is green or blue (robot position)\n",
    "- at least one of the pixels is red (goal position)\n",
    "\n",
    "The output is 1 if:\n",
    "- at least one of the pixels is yellow (obstacle line) and there are no other colors present.\n",
    "\n",
    "```python\n",
    "def discretize_map(final_seg):\n",
    "    map_arr = final_seg.copy()\n",
    "    size_x, size_y, _ = map_arr.shape\n",
    "    kernel = 4\n",
    "\n",
    "    # Everything occupied at first (yellow)\n",
    "    path_arr = np.zeros([size_x//(2*kernel+1), size_y//(2*kernel+1)], np.uint8)\n",
    "    path_x, path_y = path_arr.shape\n",
    "\n",
    "    end_patch = []\n",
    "    start_patch_green = []\n",
    "    start_patch_blue = []\n",
    "\n",
    "    for x in range(path_x):\n",
    "        for y in range(path_y):\n",
    "            patch = (x,y)\n",
    "            patch_pixels = map_arr[x*(2*kernel+1)-kernel:x*(2*kernel+1)+kernel+1, y*(2*kernel+1)-kernel:y*(2*kernel+1)+kernel+1, :]\n",
    "\n",
    "            (has_red, has_green, has_blue, has_yellow) = check_patch(patch_pixels)\n",
    "\n",
    "            # check for red\n",
    "            if has_red:\n",
    "                end_patch.append(list(patch))\n",
    "                path_arr[patch] = 0\n",
    "            # check for blue\n",
    "            elif has_blue:\n",
    "                start_patch_blue.append(list(patch))\n",
    "                path_arr[patch] = 0\n",
    "            # check for green\n",
    "            elif has_green:\n",
    "                start_patch_green.append(list(patch))\n",
    "                path_arr[patch] = 0\n",
    "            # check for yellow\n",
    "            elif has_yellow:\n",
    "               path_arr[patch] = 1\n",
    "```\n",
    "\n",
    "The result of this filtering is a map of typical size 120 by 60 pixels (depending on the size of the input map and the kernel) where the free, start and goal space is 0 and the obstacle space is 1 as well as the three lists with all the green and blue start patches and the goal patch.<br>\n",
    "This reduced map and the start and goal positions are fed into the A* algorithm that calculates the optimal global path from the start to the goal. However, sometimes the vision module does not work perfectly and produces some colors outside of the corresponding objects (i.e. a red pixel outside of the goal square). To remove the unwanted pixels, the three patches (start and goal) are reduced to plus-minus two standard deviations of the coordinates. This means that the five percent of pixels furthest from the mean are deleted.<br>\n",
    "The path calculated by A* is a series of coordinates in the reduced map format. To serve as directions for the Thymio, these coordinates need to be resized to the original map (multiplied by the reduction factor $2\\:\\times$ ``kernel`` $+\\:1$).<br>\n",
    "Additionnally, the path is filtered in order to keep only the corner points and delete the points on straight lines. This facilitates the path following and makes it more smooth. The function **only_corners_path** does the filtering. We also remove the first point of the path which corresponds to the starting point. This helps avoiding weird behaviours when the robots turn around at the beginning to find its own starting position.\n",
    "\n",
    "```python\n",
    "def only_corners_path(path):\n",
    "    test_if_aligned = list(path.copy())\n",
    "    only_corners_path = list()\n",
    "    while len(test_if_aligned) > 2:\n",
    "        x_a = test_if_aligned[1][0] - test_if_aligned[0][0]\n",
    "        y_a = test_if_aligned[1][1] - test_if_aligned[0][1]\n",
    "        x_b = test_if_aligned[2][0] - test_if_aligned[1][0]\n",
    "        y_b = test_if_aligned[2][1] - test_if_aligned[1][1]\n",
    "        norm_a = np.sqrt(x_a**2 + y_a**2)\n",
    "        norm_b = np.sqrt(x_b**2 + y_b**2)\n",
    "        normalized_dot_product = (x_a * x_b + y_a * y_b) / (norm_a * norm_b)\n",
    "        if np.abs(normalized_dot_product - 1) < PATHFINDING_TOL:\n",
    "            test_if_aligned.pop(1)\n",
    "        else:\n",
    "            only_corners_path.append(test_if_aligned.pop(0))\n",
    "    assert len(test_if_aligned) == 2\n",
    "    only_corners_path = only_corners_path + test_if_aligned\n",
    "    # Delete starting point to avoid weird moves at first (especially getting out of map)\n",
    "    only_corners_path.pop(0)\n",
    "    return np.array(only_corners_path)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Control\n",
    "The class ThymioControl in the file ``control.py`` solely handles the interactions between the PC and the Thymio through the ``tdmclient`` library, and contains the related higher-level algorithms for path following and obstacle avoidance. It thus controls the speeds of the robotâs wheels depending on the robot coordinates given by the Kalman filtering module, and then sends the speeds it applied back to the latter.\n",
    "\n",
    "```python\n",
    "def __init__(self):\n",
    "    self.client = ClientAsync()\n",
    "    self.node = aw(self.client.wait_for_node())\n",
    "    aw(self.node.lock())\n",
    "    self.position = [0,0]\n",
    "    self.angle = 0\n",
    "    self.speed_target = (0,0)\n",
    "# path following\n",
    "    self.stop_planned = False\n",
    "    self.path = [(0,0)]\n",
    "    self.path_index = 0\n",
    "# obstacle avoidance\n",
    "    self.proxs = np.zeros(5)\n",
    "    self.obst_direction = 0\n",
    "# timers\n",
    "    self.stop_timer = Timer(0, self.stop)\n",
    "    self.move_timer = RepeatedTimer(MOVE_INTERVAL, self.navigation)\n",
    "```\n",
    "\n",
    "The function ``navigation()``, controlling both path following and obstacle avoidance, is called by the RepeatedTimer ``move_timer`` at a period ``MOVE_INTERVAL``. This function first reads the values of the proximity sensors of the Thymio, and then starts either an obstacle avoidance step if a significant value is read in front of the robot, or a path following step if not. This switch is controlled by the value of ``PROX_THRESHOLD``, which is set too 1000, or around 65 millimeters, for our application. The timer interval ``MOVE_INTERVAL`` is set to ``0.21`` seconds to leave time for both fetching of the sensor values and setting new motor values, both having a set duration of 100 milliseconds. ``move_timer`` is started by the function ``follow_path()``, which is called from the main program.\n",
    "\n",
    "```python\n",
    "def navigation(self):\n",
    "    self.get_prox()\n",
    "    if np.any(self.proxs > PROX_THRESHOLD):\n",
    "        self.avoid_obstacles()\n",
    "    else: self.move_to_goal()\n",
    "\n",
    "def follow_path(self):\n",
    "    self.move_timer.start()\n",
    "```\n",
    "\n",
    "The most basic movement function is ``move(l_speed, r_speed)`` which simply sends the desired speeds to the Thymio and records the current speed-target. This communication with the robot has a standard duration of 100 milliseconds, which makes an efficient PID controller running on the computer impossible. A PID running on the Thymio itself was also not desirable due to the frequent sending of observed positions and fetching of estimated positions, requiring event-listeners on both machines, adding more delays to the movement. Running a PID and odometry on the Thymio itself is further impeded by the robot's inability to calculate with floating point values (running odometry on the computer instead would require speed-updates at a very high frequency, which is completely impossible due to the previous reasons).\n",
    "\n",
    "We thus decided to greatly reduce the amount of communications by setting a constant speed for all movements and estimating its duration; the commands sent to the robot are now only the initial start of the movement, and the stopping command after a set duration. In this case, no (relevant) code is flashed onto the Thymio.\n",
    "\n",
    "```python\n",
    "def move(self, l_speed, r_speed=None):\n",
    "    if r_speed is None: r_speed = l_speed\n",
    "    aw(self.node.set_variables({\"motor.left.target\": [int(l_speed)],\n",
    "                \"motor.right.target\": [int(r_speed)]}))\n",
    "    self.speed_target = (l_speed, r_speed)\n",
    "\n",
    "```\n",
    "\n",
    "### Path following\n",
    "The simple path following algorithm function ``move_to_goal()`` can be reduced to the following pseudo-code:\n",
    "\n",
    "<pre>\n",
    "calculate distance to goal  \n",
    "if distance is above threshold:  \n",
    "    calculate angle to goal\n",
    "    if angle is above threshold:\n",
    "        <i>re-orient robot</i>\n",
    "    else:\n",
    "        <i>move robot</i>\n",
    "else:\n",
    "    increment goal index in path array\n",
    "    if index surpasses the length of the path:\n",
    "        stop the movement timer\n",
    "</pre>\n",
    "\n",
    "Both italic lines represent a movement of the robot. For these functions, the movement duration is first estimated using the real-world speed conversion constant ``SPEED_TO_MMS``, which has been experimentally found to be $0.32\\:(mm/s)^{-1}$ (at our chosen speed of ``STANDARD_SPEED = 200``, the Thymio moves 32 centimeters in 5 seconds). This duration is capped at 0.5 seconds to allow re-orientation in case of an asynchronous Kalman update.\n",
    "\n",
    "The distance threshold ``DIST_TOL`` determines the point at which a goal is reached, while the angle threshold ``ANGLE_TOL`` triggers a re-orientation of the robot on its way to a goal. Their values were set to 10 millimeters and 0.1 rad = 5.73Â° respectively, which allows precise enough movement and good path following without any issues.\n",
    "\n",
    "```python\n",
    "def move_to_goal(self):\n",
    "    if self.stop_planned: return\n",
    "    goal = self.path[self.path_index]\n",
    "    dist = math.sqrt((goal[0] - self.position[0])**2 + (goal[1] - self.position[1])**2)\n",
    "    if dist > DIST_TOL:\n",
    "        angle = (math.atan2(goal[1] - self.position[1], goal[0] - self.position[0])\n",
    "                    - self.angle + math.pi) % (2*math.pi) - math.pi\n",
    "        if abs(angle) > ANGLE_TOL:\n",
    "            direction = 1 if angle > 0 else -1 # 1 = turn left, -1 = turn right\n",
    "            t = abs(angle)*WHEEL_DIST / (2*STANDARD_SPEED*SPEED_TO_MMS)\n",
    "            t = min(t, MAX_TIME) # allow Kalman updates\n",
    "            self.timed_move(direction*STANDARD_SPEED, -direction*STANDARD_SPEED, t)\n",
    "        else:\n",
    "            t = dist / (STANDARD_SPEED*SPEED_TO_MMS)\n",
    "            t = min(t, MAX_TIME) # allow Kalman updates\n",
    "            self.timed_move(STANDARD_SPEED, STANDARD_SPEED, t)\n",
    "    else:\n",
    "        self.path_index += 1\n",
    "        if self.path_index >= len(self.path): self.end_path()\n",
    "```\n",
    "\n",
    "The function shown above lacks a few lines at its beginning, which handle the end of the obstacle avoidance procedure; these will be shown and addressed in the corresponding section.\n",
    "\n",
    "The final move duration is used to program ``stop_timer``, which simply stops the robot and resets the boolean ``stop_planned`` to allows re-entry into ``move_to_goal()``. The timer is programmed by the function ``timed_move()``, which also sets ``stop_planned`` that prohibits any new command from ``move_to_goal()``. Since the ``stop()`` function needs to send new motor-targets to the Thymio, the communication delay is first subtracted from the timer duration. \n",
    "\n",
    "```python\n",
    "def stop(self):\n",
    "    self.stop_planned = False\n",
    "    self.move(0)\n",
    "    \n",
    "def timed_move(self, l_speed, r_speed, time):\n",
    "    self.stop_planned = True\n",
    "    self.stop_timer = Timer(time - 0.1, self.stop)  # compensate stopping delay\n",
    "    self.move(l_speed, r_speed)\n",
    "    self.stop_timer.start()\n",
    "```\n",
    "\n",
    "The ``end_path()`` function at the very end of ``move_to_goal()`` stops the robot and the movement timers.\n",
    "\n",
    "```python\n",
    "def end_path(self):\n",
    "    self.move_timer.stop()\n",
    "    self.stop_timer.cancel()\n",
    "    self.stop()\n",
    "    self.crab_rave()\n",
    "```\n",
    "\n",
    "The latter is the only function in the project that sends Aseba-code to the Thymio, which crudely plays the refrain of the electronic track \"Crab Rave\" by producer \"Noisestorm\" until the center button is pressed. Finally, the remaining function of the class, called ``keyboard()``, allows the user to control a Thymio's wheel speeds using their computer's WASD keys, albeit with the usual 100 millisecond delay between input and action. This code was used to control another Thymio on the map during testing and the demonstration video.\n",
    "\n",
    "```python\n",
    "def keyboard(self):\n",
    "    import keyboard\n",
    "    while True:\n",
    "        if keyboard.is_pressed('q'):\n",
    "            thymio.stop(); break\n",
    "        vl = 0; vr = 0\n",
    "        if keyboard.is_pressed('w'):\n",
    "            vl += 250; vr += 250\n",
    "        if keyboard.is_pressed('s'):\n",
    "            vl -= 250; vr -= 250\n",
    "        if keyboard.is_pressed('a'):\n",
    "            vl -= 250; vr += 250\n",
    "        if keyboard.is_pressed('d'):\n",
    "            vl += 250; vr -= 250\n",
    "        thymio.move(vl, vr)\n",
    "\n",
    "def crab_rave(self):\n",
    "    program = '''\n",
    "var note[19] = [2349, 1976, 1568, 1568, 2349, 2349, 1760, 1397, 1397, 2349, 2349, 1760, 1397, 1397, 2094, 2094, 1319, 1319, 1397]\n",
    "var duration[19] = [2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2]\n",
    "var i = 1\n",
    "var play = 1\n",
    "call sound.freq(note[0]/2, 8*duration[0])\n",
    "onevent sound.finished\n",
    "if play == 1 then\n",
    "    call sound.freq(note[i]/2, 8*duration[i])\n",
    "    i = i + 1\n",
    "    if i == 19 then\n",
    "        i = 0\n",
    "    else\n",
    "    end\n",
    "end\n",
    "onevent button.center\n",
    "play = 0\n",
    "    '''\n",
    "    async def prog():\n",
    "        await self.node.compile(program)\n",
    "        await self.node.run()\n",
    "    self.client.run_async_program(prog)\n",
    "```\n",
    "\n",
    "### Obstacle avoidance\n",
    "In case an obstacle is detected by the front proximity sensors, ``navigation()`` calls the function ``avoid_obstacles()``. This function first stops ``stop_timer`` and its boolean in case the path following function has activated it, then calculates the speeds of both wheels using a dot product on coefficients. These coefficients make the robot turn if the obstacle is detected on the sides, and move backwards when it lies right in front of the Thymio (since the readings are practically never symmetrical, the middle coefficient has never led to a draw by repetition). Furthermore, if the obstacle is only found at the extremities of the sensed space, the robot will keep moving forward at half its standard speed. The side on which the obstacle has been found is then saved in ``obst_direction``, the speed sent to the robot, and the function starts anew while ``navigation()`` detects a significant presence in front of the robot.\n",
    "\n",
    "```python\n",
    "def avoid_obstacles(self):\n",
    "    self.stop_planned = False\n",
    "    self.stop_timer.cancel()\n",
    "    speed = np.dot(self.proxs, [[3, -3], [1, -1], [-1, -1], [-1, 1], [-3, 3]])/100\n",
    "    if (np.sum(self.proxs[1:4]) < 30): # if no obstacle in front, move forward\n",
    "       speed[0] += STANDARD_SPEED/2; speed[1] += STANDARD_SPEED/2\n",
    "    self.obst_direction = 1 if speed[0] < speed[1] else -1 # go left when obstacle on left and vice versa\n",
    "    self.move(speed[0], speed[1])\n",
    "```\n",
    "\n",
    "Once the obstacle disappears or is completely avoided, the robot will re-enter ``move_to_goal()`` and use the last computed value of ``obst_direction`` to spurt forward while slightly turning towards the obstacle for ``OBST_TIME = 1`` seconds, in hopes of landing beyond it. This value along with ``OBST_SPEED`` and ``OBST_TURN_SPEED`` has been calibrated for driving past another immobile Thymio. It must be noted that the yellow lines of the map are ignored for the entire avoidance sequence, and also once the path following sets in again; these markings are only used during the pathfinding to place the checkpoints the robot has to travel through. The Thymio will also keep circling around the obstacle while the latter covers its next checkpoint.\n",
    "\n",
    "Here are the lines of code of ``move_to_goal()`` that weren't shown in the previous section:\n",
    "\n",
    "```python\n",
    "def move_to_goal(self):\n",
    "    if self.stop_planned: return\n",
    "    if self.obst_direction != 0: # avoid previously detected obstacle\n",
    "        self.timed_move(self.obst_direction*OBST_TURN_SPEED + OBST_SPEED,\n",
    "                        -self.obst_direction*OBST_TURN_SPEED + OBST_SPEED, OBST_TIME)\n",
    "        self.obst_direction = 0\n",
    "        return\n",
    "    goal = self.path[self.path_index] # start of path following code\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Filter\n",
    "The Kalman Filter is quite standard. It is a class with the following attributes:\n",
    "- the current state estimation, a vector with (x position, y position, orientation theta)\n",
    "- the current state estimation covariance matrix\n",
    "- the process uncertainty matrix (constant, describes the noise of the state propagation)\n",
    "- the measurement uncertainty matrix (constant, describes the noise of the measurement by the camera)\n",
    "- the observation matrix (constant, maps the state to the observation of the state), since all of the states can be measured, this is an identity matrix\n",
    "- the time stamp of the last time the filter was called (used for calculating the elapsed time)\n",
    "\n",
    "```python\n",
    "class Kalman:\n",
    "    def __init__(self):\n",
    "        # timestep for state propagation\n",
    "        self.dt = None\n",
    "        self.prev_time = None\n",
    "        # state\n",
    "        self.x = np.zeros((3, 1)) # state\n",
    "        self.P = 1000*np.ones((3,3)) # state covariance\n",
    "        # process noise\n",
    "        self.R = np.diag([NOISE_POS_XY, NOISE_POS_XY, NOISE_POS_THETA])\n",
    "        # measurement noise\n",
    "        self.Q = np.diag([NOISE_MEASURE_XY, NOISE_MEASURE_XY])\n",
    "        # Observation Matrix H\n",
    "        self.H = np.array([[1, 0, 0],\n",
    "                           [0, 1, 0],\n",
    "                           [0, 0, 1]])\n",
    "```\n",
    "\n",
    "and the following methods:\n",
    "- state_prop <br>\n",
    "It takes as input the speeds of the two wheels during the last timestep. From that it estimates the new position and state covariance matrix and updates the corresponding attributes. Because the state propagation includes sinus and cosinus calculations, it is not a linear system. To facilitate the calculations a first order approximation is used that linearizes the system. The error of the approximation is limited because the state_prop function is called every 25ms. So, the involved angle is very small and so is the error of the linearization.\n",
    "\n",
    "```python\n",
    "def state_prop(self, u):\n",
    "        if self.prev_time is None: # initialisation\n",
    "            self.prev_time = time.time()\n",
    "            return\n",
    "        u = np.array(u)\n",
    "        self.dt = time.time() - self.prev_time\n",
    "        self.prev_time = time.time()\n",
    "        # https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/764fafce112bed6482c61f1593bd0977_odomtutorial.pdf\n",
    "        (dx, dy) = self.dt*SPEED_TO_MMS*u # left and right displacements [mm]\n",
    "        da = -(dy - dx)/WHEEL_DIST # rotation angle [rad]\n",
    "        dc = (dx + dy)/2 # center displacement [mm]\n",
    "        (vx, vy) = SPEED_TO_MMS*u # left and right wheel speeds [mm/s]\n",
    "        vt = (vx + vy)/2 # translation speed [mm/s]\n",
    "        vr = -(vy - vx)/WHEEL_DIST # rotation speed [rad/s]\n",
    "        #print(\"angle is \", self.x[2])\n",
    "        sin = math.sin(self.x[2])\n",
    "        cos = math.cos(self.x[2])\n",
    "        # state propagation\n",
    "        self.x[0] = self.x[0] + dc*cos\n",
    "        self.x[1] = self.x[1] + dc*sin\n",
    "        self.x[2] = (self.x[2] + da) % (2*math.pi)\n",
    "        # transition function (state propagation matrix)\n",
    "        A = np.array([[1, 0, -self.dt*vt*sin],\n",
    "                      [0, 1,  self.dt*vt*cos],\n",
    "                      [0, 0, 1]])\n",
    "        # input transition matrix\n",
    "        L = np.array([[self.dt*vx/2*cos, -self.dt*vy/2*cos],\n",
    "                      [self.dt*vx/2*cos, -self.dt*vx/2*sin],\n",
    "                      [self.dt*vx/WHEEL_DIST, -self.dt*vy/WHEEL_DIST]])\n",
    "        # state covariance propagation\n",
    "        self.P = A@self.P@A.T + L@self.Q@L.T\n",
    "```\n",
    "- state_correct <br>\n",
    "It takes as input the measurement of the position and angle of the robot from the camera. It calculates the Kalman gain and corrects the state and state covariance. It takes quite some time to take an image with the camera and extract the position of the robot, which means that the correct function can only be called every 1.0s.\n",
    "\n",
    "```python\n",
    "def state_correct(self, z):\n",
    "        z = np.reshape(z, (3,1))\n",
    "        K = self.P@self.H.T@np.linalg.inv(self.H@self.P@self.H.T + self.R) # kalman gain\n",
    "        self.x = self.x + K@(z - self.H@self.x) # state\n",
    "        self.P = (np.eye(3) - K@self.H)@self.P # state covariance\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "All the main modules of the project were implemented and tested. The Thymio can operate on a parking lot with two lines of spaces. This includes finding the goal parking lot, navigating around the other spaces, avoid unforeseen obstacles such as other cars.\n",
    "\n",
    "### Possible improvements\n",
    "- The pathfinding algorithm tends to return a jagged path, which can lead to unnecessary movements of the robot even after removing most of the intermediary points. This could be improved by adding a turning penalty to the A* algorithm.\n",
    "- The obstacle avoidance is very basic, and assumes the next checkpoint is beyond the obstacle. Since this is not always the case, the robot might spurt past the obstacle and then have to turn around. The spurt could be replaced by a mixed movement, composed of both obstacle avoidance and path following.\n",
    "- The path following is also very crude: it only tries to reach the next checkpoint and does not take into account the checkpoints after that. This means that the robot might overshoot the checkpoint and then turn around, even though it would be faster to carry on to the next one. Similarly, after going off-path during the obstacle avoidance, rejoining the path at a later checkpoint might be faster. This might be achieved by, for example, projecting the current position onto the path.\n",
    "- The Kalman filter obviously suffers from an offset on the position due to perspective, since the reference points on the Thymio are not on the ground plane. This leads to the robot driving closer to the center of the map than it should during path following, and some Kalman corrections confusing the odometric estimations. This could be solved with this trigonometric offset being taken into account in the state correction function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from RepeatedTimer import RepeatedTimer\n",
    "from control import ThymioControl\n",
    "from Kalman import Kalman\n",
    "from parking_segmentation import *\n",
    "from color_segmentation import *\n",
    "from color_centroids import *\n",
    "from discretize_map import *\n",
    "from constants import *\n",
    "\n",
    "# chose the camera\n",
    "id_camera = 1\n",
    "# parking segmentation\n",
    "corners, destination_corners = set_parking_limits(id_camera)\n",
    "# color segmentation\n",
    "segmentation, refined_color_dict_HSV, kernels, openings = get_color_mask(id_camera, corners,\n",
    "            destination_corners, real_size=(NOMINAL_AREA_LENGTH, NOMINAL_AREA_WIDTH))\n",
    "cv.namedWindow(\"Segmentation Result\")\n",
    "cv.imshow(\"Segmentation Result\", segmentation)\n",
    "key = cv.waitKey(0)\n",
    "cv.destroyWindow(\"Segmentation Result\")\n",
    "# thymio and objective localization\n",
    "centroids = {'goal': (0, 0), 'thymio': (0, 0), 'green': (0, 0), 'blue': (0, 0)}\n",
    "theta_thymio = 0; localization = None\n",
    "\n",
    "# main control classes\n",
    "kalman = Kalman(); thymio = ThymioControl()\n",
    "\n",
    "def compute_centroids():\n",
    "    global centroids, theta_thymio, localization\n",
    "    centroids, theta_thymio, localization = get_centroids(id_camera, corners, destination_corners, refined_color_dict_HSV,\n",
    "                kernels, openings, prev_centroids=centroids, real_size=(NOMINAL_AREA_LENGTH, NOMINAL_AREA_WIDTH), real_time=False)\n",
    "    kalman.state_correct(np.array([centroids['thymio'][0], centroids['thymio'][1], theta_thymio]))\n",
    "    thymio.position = (kalman.x[0, 0], kalman.x[1, 0])\n",
    "    thymio.angle = kalman.x[2, 0]\n",
    "\n",
    "def odometry():\n",
    "    kalman.state_prop(thymio.speed_target)\n",
    "    thymio.position = (kalman.x[0, 0], kalman.x[1, 0])\n",
    "    thymio.angle = kalman.x[2, 0]\n",
    "    #print(thymio.position[0], thymio.position[1], thymio.angle*180/math.pi)\n",
    "\n",
    "# initialise position and path\n",
    "compute_centroids()\n",
    "kalman.set_state((centroids['thymio'][0], centroids['thymio'][1], theta_thymio))\n",
    "thymio.position = (centroids['thymio'][0], centroids['thymio'][1])\n",
    "thymio.angle = theta_thymio\n",
    "path = discretize_map(segmentation)\n",
    "thymio.set_path(path)\n",
    "\n",
    "# start updating position and follow path (main loop)\n",
    "image_timer = RepeatedTimer(IMAGE_PROCESSING_INTERVAL, compute_centroids)\n",
    "odometry_timer = RepeatedTimer(ODOMETRY_INTERVAL, odometry)\n",
    "image_timer.start()\n",
    "odometry_timer.start()\n",
    "thymio.follow_path()\n",
    "\n",
    "# start displaying position\n",
    "cv.namedWindow(\"Localization Result\")\n",
    "while True:\n",
    "    cv.imshow(\"Localization Result\", localization)\n",
    "    key = cv.waitKey(15)\n",
    "    if key == ord('q'): break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "8540791a84c9dd273039c82e4c4906b2f22bd80b7ff15c37ffcc5926b6e4ab9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
